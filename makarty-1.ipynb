{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system as bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bash(\"\\\n",
    "POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py \\\n",
    ">> log-test_synt_affinity2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "for i in `seq 10`;do echo $i; sleep 1;done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "from IPython.core.magic import register_line_magic\n",
    "\n",
    "\n",
    "@register_line_magic\n",
    "def runrealcmd(command):\n",
    "    process = Popen(command, stdout=PIPE, shell=True, stderr=STDOUT, bufsize=1, close_fds=True)\n",
    "    for line in iter(process.stdout.readline, b''):\n",
    "        print(line.rstrip().decode('utf-8'))\n",
    "    process.stdout.close()\n",
    "    process.wait()\n",
    "\n",
    "@register_line_magic\n",
    "def tox(command):\n",
    "    process = Popen(command, stdout=PIPE, shell=True, stderr=STDOUT, bufsize=1, close_fds=True)\n",
    "    counter = 0\n",
    "    for line in iter(process.stdout.readline, b''):\n",
    "        counter += 1\n",
    "        if counter <= 135: continue\n",
    "        print(line.rstrip().decode('utf-8'))\n",
    "    process.stdout.close()\n",
    "    process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PING www.google.com (74.125.201.106) 56(84) bytes of data.\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=1 ttl=52 time=1.20 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=2 ttl=52 time=1.67 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=3 ttl=52 time=1.57 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=4 ttl=52 time=1.33 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=5 ttl=52 time=1.17 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=6 ttl=52 time=1.49 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=7 ttl=52 time=1.41 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=8 ttl=52 time=1.29 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=9 ttl=52 time=1.45 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=10 ttl=52 time=1.23 ms\n",
      "\n",
      "--- www.google.com ping statistics ---\n",
      "10 packets transmitted, 10 received, 0% packet loss, time 9014ms\n",
      "rtt min/avg/max/mdev = 1.179/1.386/1.679/0.163 ms\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd ping -c10 www.google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd for i in `seq 10`;do echo $i; sleep 1;done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodledev inst-nodeps: /home/vasily/artem/kubectl-val/.tox/.tmp/package/5/kubectl-val-0.1.4.tar.gz\n",
      "poodledev installed: aiohttp==3.6.2,altgraph==0.16.1,apipkg==1.5,appdirs==1.4.3,astroid==2.3.3,async-timeout==3.0.1,attrs==19.3.0,bowler==0.8.0,bsdiff4==1.1.5,Cerberus==1.3.2,certifi==2019.11.28,chardet==3.0.4,Click==7.0,coverage==4.5.4,dephell==0.7.9,dephell-archive==0.1.5,dephell-discover==0.2.10,dephell-licenses==0.1.6,dephell-links==0.1.4,dephell-markers==1.0.2,dephell-pythons==0.1.12,dephell-setuptools==0.2.1,dephell-shells==0.1.3,dephell-specifier==0.2.1,dephell-venvs==0.1.17,dephell-versioning==0.1.1,docker==4.1.0,dockerpty==0.4.1,docutils==0.15.2,dsdev-utils==1.0.4,ed25519==1.5,execnet==1.7.1,fissix==19.2b1,Flask==1.1.1,flatdict==3.4.0,gitdb2==2.0.6,GitPython==3.0.5,html5lib==1.0.1,idna==2.8,importlib-metadata==1.1.0,isort==4.3.21,itsdangerous==1.1.0,Jinja2==2.10.3,kubectl-val==0.1.4,lazy-object-proxy==1.4.3,logzero==1.5.0,m2r==0.2.1,MarkupSafe==1.1.1,mccabe==0.6.1,mistune==0.8.4,more-itertools==8.0.0,multidict==4.6.1,packaging==19.2,pbr==5.4.4,pexpect==4.7.0,pluggy==0.13.1,poodle==0.1.10,ptyprocess==0.6.0,py==1.8.0,Pygments==2.5.2,PyInstaller==3.5,pylint==2.4.4,pyparsing==2.4.5,pytest==5.3.1,pytest-cache==1.0,pytest-cov==2.8.1,pytest-pylint==0.14.1,PyUpdater==3.1.1,PyYAML==5.2,requests==2.22.0,ruamel.yaml==0.16.5,ruamel.yaml.clib==0.2.0,sh==1.12.14,shellingham==1.3.1,six==1.13.0,smmap2==2.0.5,stevedore==1.31.0,tabulate==0.8.6,tomlkit==0.5.8,typed-ast==1.4.0,urllib3==1.25.7,wcwidth==0.1.7,webencodings==0.5.1,websocket-client==0.56.0,Werkzeug==0.16.0,wrapt==1.11.2,yarl==1.4.1,yaspin==0.15.0,zipp==0.6.0\n",
      "poodledev run-test-pre: PYTHONHASHSEED='3725075330'\n",
      "poodledev run-test: commands[0] | bash -c 'cd ../poodle && dephell project build --from pyproject.toml'\n",
      "WARNING cannot find tool.dephell section in the config (path=pyproject.toml)\n",
      "INFO dumping... (format=setuppy)\n",
      "INFO dumping... (format=egginfo)\n",
      "INFO dumping... (format=sdist)\n",
      "INFO dumping... (format=wheel)\n",
      "INFO builded\n",
      "poodledev run-test: commands[1] | pip uninstall -y poodle\n",
      "Uninstalling poodle-0.1.10:\n",
      "  Successfully uninstalled poodle-0.1.10\n",
      "poodledev run-test: commands[2] | bash -c 'cd ../poodle && python ./setup.py install'\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing poodle.egg-info/PKG-INFO\n",
      "writing dependency_links to poodle.egg-info/dependency_links.txt\n",
      "writing entry points to poodle.egg-info/entry_points.txt\n",
      "writing requirements to poodle.egg-info/requires.txt\n",
      "writing top-level names to poodle.egg-info/top_level.txt\n",
      "reading manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "writing manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/poodle_main.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/schedule.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/arithmetic.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/pddlSplitter.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/web_solver.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/__init__.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/problem.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/poodle_main.py to poodle_main.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/schedule.py to schedule.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/arithmetic.py to arithmetic.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/pddlSplitter.py to pddlSplitter.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/web_solver.py to web_solver.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/problem.py to problem.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "poodle.__pycache__.poodle_main.cpython-37: module references __file__\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getsourcefile\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.findsource\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getframeinfo\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getouterframes\n",
      "creating 'dist/poodle-0.1.10-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing poodle-0.1.10-py3.7.egg\n",
      "creating /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Extracting poodle-0.1.10-py3.7.egg to /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Adding poodle 0.1.10 to easy-install.pth file\n",
      "Installing poodleserver script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Installed /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Processing dependencies for poodle==0.1.10\n",
      "Searching for requests==2.22.0\n",
      "Best match: requests 2.22.0\n",
      "Adding requests 2.22.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Flask==1.1.1\n",
      "Best match: Flask 1.1.1\n",
      "Adding Flask 1.1.1 to easy-install.pth file\n",
      "Installing flask script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for idna==2.8\n",
      "Best match: idna 2.8\n",
      "Adding idna 2.8 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for chardet==3.0.4\n",
      "Best match: chardet 3.0.4\n",
      "Adding chardet 3.0.4 to easy-install.pth file\n",
      "Installing chardetect script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for certifi==2019.11.28\n",
      "Best match: certifi 2019.11.28\n",
      "Adding certifi 2019.11.28 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for urllib3==1.25.7\n",
      "Best match: urllib3 1.25.7\n",
      "Adding urllib3 1.25.7 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Jinja2==2.10.3\n",
      "Best match: Jinja2 2.10.3\n",
      "Adding Jinja2 2.10.3 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Werkzeug==0.16.0\n",
      "Best match: Werkzeug 0.16.0\n",
      "Adding Werkzeug 0.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for itsdangerous==1.1.0\n",
      "Best match: itsdangerous 1.1.0\n",
      "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Click==7.0\n",
      "Best match: Click 7.0\n",
      "Adding Click 7.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for MarkupSafe==1.1.1\n",
      "Best match: MarkupSafe 1.1.1\n",
      "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Finished processing dependencies for poodle==0.1.10\n",
      "poodledev run-test: commands[3] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           548721\n",
      "poodledev run-test: commands[4] | bash -c 'cd ../downward && timeout 40000 poodleserver 2>&1 >/dev/null &'\n",
      "poodledev run-test: commands[5] | python -m pytest -vv --durations=100 --disable-pytest-warnings --pylint --pylint-jobs=4 --pylint-error-types=EF -s -x tests/test_synt_affinity.py::test_3\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.5, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /home/vasily/artem/kubectl-val/.tox/poodledev/bin/python\n",
      "cachedir: .tox/poodledev/.pytest_cache\n",
      "rootdir: /home/vasily/artem/kubectl-val\n",
      "plugins: pylint-0.14.1, cov-2.8.1\n",
      "collecting ... [D 191206 13:57:12 libs_for_tests:31] hello\n",
      "collected 1 item\n",
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "tests/test_synt_affinity.py::test_3 WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 1, pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod3\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None)\n",
      "mark_3_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, pod1=pod1, pod2=pod5, pod3=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "---  functional test : Error\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 1, pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod3\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None)\n",
      "mark_3_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, pod1=pod1, pod2=pod5, pod3=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "PASSED\n",
      "\n",
      "========================== slowest 100 test durations ==========================\n",
      "44.56s call     tests/test_synt_affinity.py::test_3\n",
      "0.00s setup    tests/test_synt_affinity.py::test_3\n",
      "0.00s teardown tests/test_synt_affinity.py::test_3\n",
      "======================== 1 passed, 1 warning in 49.48s =========================\n",
      "poodledev run-test: commands[6] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           550423\n",
      "___________________________________ summary ____________________________________\n",
      "  poodledev: commands succeeded\n",
      "  congratulations :)\n",
      "/bin/bash: line 1: 550422 Killed                  timeout 40000 poodleserver 2>&1 > /dev/null\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHtest_6_3nodes_but_needed5nodes_possible_to_add_2_nodes_suggests_thisT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodledev inst-nodeps: /home/vasily/artem/kubectl-val/.tox/.tmp/package/5/kubectl-val-0.1.4.tar.gz\n",
      "poodledev installed: aiohttp==3.6.2,altgraph==0.16.1,apipkg==1.5,appdirs==1.4.3,astroid==2.3.3,async-timeout==3.0.1,attrs==19.3.0,bowler==0.8.0,bsdiff4==1.1.5,Cerberus==1.3.2,certifi==2019.11.28,chardet==3.0.4,Click==7.0,coverage==4.5.4,dephell==0.7.9,dephell-archive==0.1.5,dephell-discover==0.2.10,dephell-licenses==0.1.6,dephell-links==0.1.4,dephell-markers==1.0.2,dephell-pythons==0.1.12,dephell-setuptools==0.2.1,dephell-shells==0.1.3,dephell-specifier==0.2.1,dephell-venvs==0.1.17,dephell-versioning==0.1.1,docker==4.1.0,dockerpty==0.4.1,docutils==0.15.2,dsdev-utils==1.0.4,ed25519==1.5,execnet==1.7.1,fissix==19.2b1,Flask==1.1.1,flatdict==3.4.0,gitdb2==2.0.6,GitPython==3.0.5,html5lib==1.0.1,idna==2.8,importlib-metadata==1.1.0,isort==4.3.21,itsdangerous==1.1.0,Jinja2==2.10.3,kubectl-val==0.1.4,lazy-object-proxy==1.4.3,logzero==1.5.0,m2r==0.2.1,MarkupSafe==1.1.1,mccabe==0.6.1,mistune==0.8.4,more-itertools==8.0.0,multidict==4.6.1,packaging==19.2,pbr==5.4.4,pexpect==4.7.0,pluggy==0.13.1,poodle==0.1.10,ptyprocess==0.6.0,py==1.8.0,Pygments==2.5.2,PyInstaller==3.5,pylint==2.4.4,pyparsing==2.4.5,pytest==5.3.1,pytest-cache==1.0,pytest-cov==2.8.1,pytest-pylint==0.14.1,PyUpdater==3.1.1,PyYAML==5.2,requests==2.22.0,ruamel.yaml==0.16.5,ruamel.yaml.clib==0.2.0,sh==1.12.14,shellingham==1.3.1,six==1.13.0,smmap2==2.0.5,stevedore==1.31.0,tabulate==0.8.6,tomlkit==0.5.8,typed-ast==1.4.0,urllib3==1.25.7,wcwidth==0.1.7,webencodings==0.5.1,websocket-client==0.56.0,Werkzeug==0.16.0,wrapt==1.11.2,yarl==1.4.1,yaspin==0.15.0,zipp==0.6.0\n",
      "poodledev run-test-pre: PYTHONHASHSEED='1193921303'\n",
      "poodledev run-test: commands[0] | bash -c 'cd ../poodle && dephell project build --from pyproject.toml'\n",
      "WARNING cannot find tool.dephell section in the config (path=pyproject.toml)\n",
      "INFO dumping... (format=setuppy)\n",
      "INFO dumping... (format=egginfo)\n",
      "INFO dumping... (format=sdist)\n",
      "INFO dumping... (format=wheel)\n",
      "INFO builded\n",
      "poodledev run-test: commands[1] | pip uninstall -y poodle\n",
      "Uninstalling poodle-0.1.10:\n",
      "  Successfully uninstalled poodle-0.1.10\n",
      "poodledev run-test: commands[2] | bash -c 'cd ../poodle && python ./setup.py install'\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing poodle.egg-info/PKG-INFO\n",
      "writing dependency_links to poodle.egg-info/dependency_links.txt\n",
      "writing entry points to poodle.egg-info/entry_points.txt\n",
      "writing requirements to poodle.egg-info/requires.txt\n",
      "writing top-level names to poodle.egg-info/top_level.txt\n",
      "reading manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "writing manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/poodle_main.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/schedule.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/arithmetic.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/pddlSplitter.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/web_solver.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/__init__.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/problem.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/poodle_main.py to poodle_main.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/schedule.py to schedule.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/arithmetic.py to arithmetic.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/pddlSplitter.py to pddlSplitter.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/web_solver.py to web_solver.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/problem.py to problem.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "poodle.__pycache__.poodle_main.cpython-37: module references __file__\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getsourcefile\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.findsource\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getframeinfo\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getouterframes\n",
      "creating 'dist/poodle-0.1.10-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing poodle-0.1.10-py3.7.egg\n",
      "creating /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Extracting poodle-0.1.10-py3.7.egg to /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Adding poodle 0.1.10 to easy-install.pth file\n",
      "Installing poodleserver script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Installed /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Processing dependencies for poodle==0.1.10\n",
      "Searching for requests==2.22.0\n",
      "Best match: requests 2.22.0\n",
      "Adding requests 2.22.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Flask==1.1.1\n",
      "Best match: Flask 1.1.1\n",
      "Adding Flask 1.1.1 to easy-install.pth file\n",
      "Installing flask script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for certifi==2019.11.28\n",
      "Best match: certifi 2019.11.28\n",
      "Adding certifi 2019.11.28 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for urllib3==1.25.7\n",
      "Best match: urllib3 1.25.7\n",
      "Adding urllib3 1.25.7 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for idna==2.8\n",
      "Best match: idna 2.8\n",
      "Adding idna 2.8 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for chardet==3.0.4\n",
      "Best match: chardet 3.0.4\n",
      "Adding chardet 3.0.4 to easy-install.pth file\n",
      "Installing chardetect script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Click==7.0\n",
      "Best match: Click 7.0\n",
      "Adding Click 7.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Jinja2==2.10.3\n",
      "Best match: Jinja2 2.10.3\n",
      "Adding Jinja2 2.10.3 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for itsdangerous==1.1.0\n",
      "Best match: itsdangerous 1.1.0\n",
      "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Werkzeug==0.16.0\n",
      "Best match: Werkzeug 0.16.0\n",
      "Adding Werkzeug 0.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for MarkupSafe==1.1.1\n",
      "Best match: MarkupSafe 1.1.1\n",
      "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Finished processing dependencies for poodle==0.1.10\n",
      "poodledev run-test: commands[3] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "CLEAN\n",
      "poodledev run-test: commands[4] | bash -c 'cd ../downward && timeout 40000 poodleserver 2>&1 >/dev/null &'\n",
      "poodledev run-test: commands[5] | python -m pytest -vv --durations=100 --disable-pytest-warnings --pylint --pylint-jobs=4 --pylint-error-types=EF -s -x tests/test_synt_affinity.py::test_4\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.5, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /home/vasily/artem/kubectl-val/.tox/poodledev/bin/python\n",
      "cachedir: .tox/poodledev/.pytest_cache\n",
      "rootdir: /home/vasily/artem/kubectl-val\n",
      "plugins: pylint-0.14.1, cov-2.8.1\n",
      "collecting ... [D 191206 13:59:37 libs_for_tests:31] hello\n",
      "collected 1 item\n",
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "tests/test_synt_affinity.py::test_4 WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 5, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod4\n",
      "SelectNode: SelectedNode=node 3, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 1, pod1=pod4\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 5, pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 5, podStarted=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 4, node_of_pod4=node 5, pod1=pod2, pod2=pod5, pod3=pod6, pod4=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "---  functional test : Error\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 5, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod4\n",
      "SelectNode: SelectedNode=node 3, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 1, pod1=pod4\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 5, pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 5, podStarted=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod6, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 4, node_of_pod4=node 5, pod1=pod2, pod2=pod5, pod3=pod6, pod4=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "PASSED\n",
      "\n",
      "========================== slowest 100 test durations ==========================\n",
      "151.44s call     tests/test_synt_affinity.py::test_4\n",
      "0.00s setup    tests/test_synt_affinity.py::test_4\n",
      "0.00s teardown tests/test_synt_affinity.py::test_4\n",
      "=================== 1 passed, 1 warning in 156.28s (0:02:36) ===================\n",
      "poodledev run-test: commands[6] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           550726\n",
      "___________________________________ summary ____________________________________\n",
      "  poodledev: commands succeeded\n",
      "  congratulations :)\n",
      "/bin/bash: line 1: 550725 Killed                  timeout 40000 poodleserver 2>&1 > /dev/null\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHtest_6_3nodes_but_needed5nodes_possible_to_add_2_nodes_suggests_thisT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tests/test_synt_affinity.py::test_5 WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 2, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod5\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod5\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod5\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 4, pod1=pod1\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod5, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, node_of_pod4=node 4, pod1=pod2, pod2=pod6, pod3=pod5, pod4=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "---  functional test : Error\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 2, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod5\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod5\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod5\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 4, pod1=pod1\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod5, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, node_of_pod4=node 4, pod1=pod2, pod2=pod6, pod3=pod5, pod4=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "PASSED\n",
      "\n",
      "========================== slowest 100 test durations ==========================\n",
      "59.01s call     tests/test_synt_affinity.py::test_5\n",
      "0.00s setup    tests/test_synt_affinity.py::test_5\n",
      "0.00s teardown tests/test_synt_affinity.py::test_5\n",
      "======================== 1 passed, 1 warning in 59.65s =========================\n",
      "poodledev run-test: commands[6] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           540200\n",
      "___________________________________ summary ____________________________________\n",
      "  poodledev: commands succeeded\n",
      "  congratulations :)\n",
      "/bin/bash: line 1: 540199 Killed                  timeout 40000 poodleserver 2>&1 > /dev/null\n"
     ]
    }
   ],
   "source": [
    "%tox POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHtest_6_3nodes_but_needed5nodes_possible_to_add_2_nodes_suggests_thisT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "\n",
    "    # manually_initiate_killing_of_pod: pod_killed=pod5\n",
    "    # p.SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
    "    # SelectNode: SelectedNode=node 2, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
    "    # SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
    "    # KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
    "    # SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod5\n",
    "    # SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod5\n",
    "    # SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod5\n",
    "    # manually_initiate_killing_of_pod: pod_killed=pod1\n",
    "    # KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
    "    # SelectNode: SelectedNode=node 1, globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
    "    # SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 4, pod1=pod1\n",
    "    # StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod5, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
    "    # StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
    "    # SchedulerCleaned: globalVar=<kalc.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None)\n",
    "    # mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, node_of_pod4=node 4, pod1=pod2, pod2=pod6, pod3=pod5, pod4=pod1, scheduler=<kalc.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), service=test-service\n",
    "    # mark_antiaffinity_prefered_policy_met: service=test-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), node=node 5, pod1=pod1\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), pod1=pod5\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 2, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f8176d22110>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 6, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "Add_node: node=node 6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), node=node 5, pod1=pod6\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f8176d22110>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), node=node 5, pod1=pod2\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f8176d22110>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), node=node 5, podStarted=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f8176d22110>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f817703f510>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f8176d22110>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, node_of_pod4=node 5, pod1=pod1, pod2=pod5, pod3=pod6, pod4=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f8176d22110>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "---  functional test : Error\n",
      "PASSED\n",
      "\n",
      "========================== slowest 100 test durations ==========================\n",
      "595.39s call     tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this\n",
      "0.00s setup    tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this\n",
      "0.00s teardown tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this\n",
      "=================== 1 passed, 1 warning in 600.10s (0:10:00) ===================\n",
      "poodledev run-test: commands[6] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           554731\n",
      "___________________________________ summary ____________________________________\n",
      "  poodledev: commands succeeded\n",
      "  congratulations :)\n",
      "/bin/bash: line 1: 554730 Killed                  timeout 40000 poodleserver 2>&1 > /dev/null\n"
     ]
    }
   ],
   "source": [
    "%tox POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                    \n",
    "551922 vasily    20   0 6182404 5.799g  44180 R 100.0  5.7   2:10.97 pypy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "(selectnode globalvar-3567-obj pod-3663-obj node-3639-obj)\n",
    "(setdefaultcpulimitforpodbeforenodeassignment globalvar-3567-obj pod-3663-obj logsparseinteger-171-obj-num-8 node-4152-obj node-3639-obj)\n",
    "(selectnode globalvar-3567-obj pod-3869-obj node-3639-obj)\n",
    "(selectnode globalvar-3567-obj pod-3711-obj node-3639-obj)\n",
    "(setdefaultcpulimitforpodbeforenodeassignment globalvar-3567-obj pod-3711-obj logsparseinteger-171-obj-num-8 node-3845-obj node-3639-obj)\n",
    "(selectnode globalvar-3567-obj pod-3759-obj node-4152-obj)\n",
    "(setdefaultcpulimitforpodbeforenodeassignment globalvar-3567-obj pod-3869-obj logsparseinteger-171-obj-num-8 node-4176-obj node-3639-obj)\n",
    "(manually_initiate_killing_of_pod pod-3663-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj node-3639-obj pod-3663-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3663-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 node-3639-obj pod-3663-obj logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2 logsparseinteger-143-obj-num-6 sumresult-863-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 node-3639-obj pod-3663-obj logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2 logsparseinteger-143-obj-num-6 sumresult-863-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 service-3581-obj logsparseinteger-169-obj-num-4 logsparseinteger-137-obj-num-3 sumresult-576-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 service-3581-obj logsparseinteger-163-obj-num-0 sumresult-293-obj logsparseinteger-165-obj-num-1)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 service-3581-obj logsparseinteger-169-obj-num-4 logsparseinteger-137-obj-num-3)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-137-obj-num-3 sumresult-576-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 node-3639-obj logsparseinteger-171-obj-num-8 logsparseinteger-143-obj-num-6)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 node-3639-obj logsparseinteger-171-obj-num-8 logsparseinteger-143-obj-num-6)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 pod-3663-obj node-4152-obj scheduler-3563-obj statussched-85-obj logsparseinteger-163-obj-num-0 sumresult-293-obj logsparseinteger-165-obj-num-1)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 pod-3663-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 pod-3663-obj node-4152-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 node-3639-obj pod-3663-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3663-obj scheduler-3563-obj statussched-85-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 scheduler-3563-obj logsparseinteger-163-obj-num-0)\n",
    "(selectnode globalvar-3567-obj pod-3663-obj node-4176-obj)\n",
    "(add_node node-4176-obj)\n",
    "(setdefaultmemlimitforpodbeforenodeassignment globalvar-3567-obj pod-3663-obj logsparseinteger-169-obj-num-4 node-4061-obj node-4176-obj)\n",
    "(manually_initiate_killing_of_pod pod-3711-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj node-3639-obj pod-3711-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3711-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 node-3639-obj pod-3711-obj logsparseinteger-143-obj-num-6 logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-143-obj-num-6 logsparseinteger-167-obj-num-2 logsparseinteger-169-obj-num-4 sumresult-675-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 node-3639-obj pod-3711-obj logsparseinteger-143-obj-num-6 logsparseinteger-167-obj-num-2 logsparseinteger-169-obj-num-4 sumresult-675-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 service-3581-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2 sumresult-482-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 service-3581-obj logsparseinteger-165-obj-num-1 sumresult-388-obj logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 service-3581-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 node-3639-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2 sumresult-482-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 node-3639-obj logsparseinteger-143-obj-num-6 logsparseinteger-169-obj-num-4)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 node-3639-obj logsparseinteger-143-obj-num-6 logsparseinteger-169-obj-num-4)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 pod-3711-obj node-3845-obj scheduler-3563-obj statussched-85-obj logsparseinteger-165-obj-num-1 sumresult-388-obj logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 pod-3711-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 pod-3711-obj node-3845-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 node-3639-obj pod-3711-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3711-obj scheduler-3563-obj statussched-85-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 scheduler-3563-obj logsparseinteger-165-obj-num-1)\n",
    "(selectnode globalvar-3567-obj pod-3711-obj node-3639-obj)\n",
    "(setdefaultmemlimitforpodbeforenodeassignment globalvar-3567-obj pod-3711-obj logsparseinteger-171-obj-num-8 node-4128-obj node-3639-obj)\n",
    "(manually_initiate_killing_of_pod pod-3917-obj)\n",
    "(selectnode globalvar-3567-obj pod-3917-obj node-4128-obj)\n",
    "(setdefaultcpulimitforpodbeforenodeassignment globalvar-3567-obj pod-3917-obj logsparseinteger-171-obj-num-8 node-4176-obj node-4128-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj node-3845-obj pod-3917-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3917-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 node-3845-obj pod-3917-obj logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2 logsparseinteger-143-obj-num-6 sumresult-863-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 node-3845-obj pod-3917-obj logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2 logsparseinteger-143-obj-num-6 sumresult-863-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 service-3581-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1 sumresult-388-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 service-3581-obj logsparseinteger-167-obj-num-2 sumresult-482-obj logsparseinteger-137-obj-num-3)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 service-3581-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 node-3845-obj logsparseinteger-169-obj-num-4 logsparseinteger-137-obj-num-3 sumresult-576-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 node-3845-obj logsparseinteger-171-obj-num-8 logsparseinteger-143-obj-num-6)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 node-3845-obj logsparseinteger-171-obj-num-8 logsparseinteger-143-obj-num-6)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 pod-3917-obj node-4176-obj scheduler-3563-obj statussched-85-obj logsparseinteger-167-obj-num-2 sumresult-482-obj logsparseinteger-137-obj-num-3)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 pod-3917-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 pod-3917-obj node-4176-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 node-3845-obj pod-3917-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3917-obj scheduler-3563-obj statussched-85-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 scheduler-3563-obj logsparseinteger-167-obj-num-2)\n",
    "(selectnode globalvar-3567-obj pod-3917-obj node-3639-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj pod-3711-obj scheduler-3563-obj node-4128-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3711-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 pod-3711-obj node-4128-obj logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2 sumresult-298-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 logsparseinteger-167-obj-num-2 greaterequal-2311-obj logsparseinteger-171-obj-num-8)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 pod-3711-obj node-4128-obj logsparseinteger-171-obj-num-8 logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2 sumresult-298-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 logsparseinteger-167-obj-num-2 greaterequal-2311-obj logsparseinteger-171-obj-num-8)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 pod-3711-obj node-4128-obj logsparseinteger-171-obj-num-8 logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2 sumresult-298-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 pod-3711-obj node-4128-obj logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2 sumresult-298-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 pod-3711-obj scheduler-3563-obj node-3442-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2 sumresult-482-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 service-3581-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2 sumresult-482-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 node-4128-obj logsparseinteger-163-obj-num-0 sumresult-293-obj logsparseinteger-165-obj-num-1)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 node-4128-obj logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 node-4128-obj logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 pod-3711-obj service-3581-obj logsparseinteger-165-obj-num-1 sumresult-388-obj logsparseinteger-167-obj-num-2 statuspod-63-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3711-obj node-4128-obj node-3442-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 pod-3711-obj scheduler-3563-obj service-3581-obj logsparseinteger-137-obj-num-3)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-17 service-3581-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-18 pod-3711-obj statuspod-63-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-19 service-3581-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj pod-3917-obj scheduler-3563-obj node-3639-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3917-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 pod-3917-obj node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 sumresult-675-obj logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 logsparseinteger-143-obj-num-6 greaterequal-2343-obj logsparseinteger-171-obj-num-8)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 pod-3917-obj node-3639-obj logsparseinteger-171-obj-num-8 logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 sumresult-675-obj logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 logsparseinteger-143-obj-num-6 greaterequal-2343-obj logsparseinteger-171-obj-num-8)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 pod-3917-obj node-3639-obj logsparseinteger-171-obj-num-8 logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 sumresult-675-obj logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 pod-3917-obj node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 sumresult-675-obj logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 pod-3917-obj scheduler-3563-obj node-3442-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1 sumresult-388-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 service-3581-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1 sumresult-388-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 node-3639-obj logsparseinteger-167-obj-num-2 sumresult-482-obj logsparseinteger-137-obj-num-3)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 pod-3917-obj service-3581-obj logsparseinteger-167-obj-num-2 sumresult-482-obj logsparseinteger-137-obj-num-3 statuspod-63-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3917-obj node-3639-obj node-3442-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 pod-3917-obj scheduler-3563-obj service-3581-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-17 service-3581-obj logsparseinteger-165-obj-num-1 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-18 pod-3917-obj statuspod-63-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-19 service-3581-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj pod-3663-obj scheduler-3563-obj node-4061-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3663-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 pod-3663-obj node-4061-obj logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2 sumresult-487-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 logsparseinteger-169-obj-num-4 greaterequal-2099-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 pod-3663-obj node-4061-obj logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2 sumresult-487-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 logsparseinteger-169-obj-num-4 greaterequal-2099-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 pod-3663-obj node-4061-obj logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2 sumresult-487-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 pod-3663-obj node-4061-obj logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2 sumresult-487-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 pod-3663-obj scheduler-3563-obj node-3442-obj logsparseinteger-165-obj-num-1 logsparseinteger-163-obj-num-0 sumresult-293-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 service-3581-obj logsparseinteger-165-obj-num-1 logsparseinteger-163-obj-num-0 sumresult-293-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 node-4061-obj logsparseinteger-165-obj-num-1 sumresult-388-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 node-4061-obj logsparseinteger-167-obj-num-2 logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 node-4061-obj logsparseinteger-167-obj-num-2 logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 pod-3663-obj service-3581-obj logsparseinteger-137-obj-num-3 sumresult-576-obj logsparseinteger-169-obj-num-4 statuspod-63-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3663-obj node-4061-obj node-3442-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 pod-3663-obj scheduler-3563-obj service-3581-obj logsparseinteger-165-obj-num-1)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-17 service-3581-obj logsparseinteger-163-obj-num-0 logsparseinteger-137-obj-num-3)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-18 pod-3663-obj statuspod-63-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-19 service-3581-obj statusserv-93-obj)\n",
    "(schedulercleaned globalvar-3567-obj scheduler-3563-obj statussched-85-obj)\n",
    "(mark_4_pods_of_service_as_not_at_same_node node-3639-obj pod-3917-obj node-3845-obj pod-3869-obj node-4061-obj pod-3663-obj node-4128-obj pod-3711-obj service-3581-obj scheduler-3563-obj logsparseinteger-163-obj-num-0)\n",
    "(mark_antiaffinity_prefered_policy_met logsparseinteger-169-obj-num-4 service-3581-obj booleanobject-20-obj)\n",
    "; cost = 27 (general cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lanch with sas file stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), pod1=pod9\n",
      "manually_initiate_killing_of_pod: pod_killed=pod2\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7ff3fe8cd150>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "Add_node: node=node 6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), node=node 5, pod1=pod2\n",
      "Add_node: node=node 5\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7ff3fe8cd150>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), node=node 5, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7ff3fe8cd150>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), node=node 5, podStarted=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7ff3fe8cd150>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7ff3fe8cd0d0>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7ff3fe8cd150>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, node_of_pod4=node 5, pod1=pod1, pod2=pod5, pod3=pod2, pod4=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7ff3fe8cd150>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "---  functional test : Error\n",
      "PASSED\n",
      "\n",
      "========================== slowest 100 test durations ==========================\n",
      "610.65s call     tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this\n",
      "0.00s setup    tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this\n",
      "0.00s teardown tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this\n",
      "=================== 1 passed, 1 warning in 615.27s (0:10:15) ===================\n",
      "poodledev run-test: commands[6] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           728308\n",
      "___________________________________ summary ____________________________________\n",
      "  poodledev: commands succeeded\n",
      "  congratulations :)\n",
      "/bin/bash: line 1: 728307 Killed                  timeout 40000 poodleserver 2>&1 > /dev/null\n"
     ]
    }
   ],
   "source": [
    "%tox POODLE_STORE_SAS=1 \\\n",
    "POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "python ./compute-space.py /home/vasily/artem/downward/out/01003_test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this_2019-12-08_wozfrgcvgtkvltzrctqy/output.sas\n",
    "mark_5_pods_of_service_as_not_at_same_node        : 2949120\n",
    "mark_4_pods_of_service_as_not_at_same_node        : 368640\n",
    "mark_3_pods_of_service_as_not_at_same_node        : 30720\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11: 9720\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-8: 4860\n",
    "setdefaultmemlimitforpodbeforenodeassignment      : 2835\n",
    "setdefaultcpulimitforpodbeforenodeassignment      : 2835\n",
    "mark_2_pods_of_service_as_not_at_same_node        : 1920\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-10: 972\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9: 756\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2: 690\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2: 684\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8: 648\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7: 648\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5: 648\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-17: 648\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4: 648\n",
    "selectnode                                        : 396\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-9: 378\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-2: 345\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-2: 342\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-8: 324\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-7: 324\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-5: 324\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-14: 324\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14: 324\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-4: 324\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13: 270\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15: 252\n",
    "nodeoutagefinished                                : 204\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-6: 153\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-4: 153\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6: 153\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4: 153\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-12: 135\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-13: 126\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-12: 108\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-11: 108\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16: 108\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13: 108\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12: 108\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11: 108\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-7: 108\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-6: 108\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-5: 108\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9: 108\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8: 108\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10: 108\n",
    "setdefaultmemlimitforpod                          : 63\n",
    "setdefaultcpulimitforpod                          : 63\n",
    "initiate_killing_of_pod_because_of_node_outage    : 54\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0: 42\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10: 36\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0: 36\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7: 36\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6: 36\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5: 36\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14: 36\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-0: 21\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-3: 18\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-0: 18\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3: 18\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-18: 18\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-3: 18\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-13: 18\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-11: 18\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3: 18\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16: 18\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15: 12\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-15: 9\n",
    "manually_initiate_killing_of_pod                  : 9\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1: 6\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-10: 6\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12: 6\n",
    "killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1: 6\n",
    "initiate_node_outage                              : 6\n",
    "startpod_if_deployment_isnull_service_isnull_daemonset_isnull-1: 3\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-9: 3\n",
    "killpod_if_deployment_isnull_service_isnull_daemonset_isnull-1: 3\n",
    "add_node                                          : 3\n",
    "startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-19: 2\n",
    "schedulercleaned                                  : 2\n",
    "mark_antiaffinity_prefered_policy_met             : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodledev inst-nodeps: /home/vasily/artem/kubectl-val/.tox/.tmp/package/5/kubectl-val-0.1.4.tar.gz\n",
      "poodledev installed: aiohttp==3.6.2,altgraph==0.16.1,apipkg==1.5,appdirs==1.4.3,astroid==2.3.3,async-timeout==3.0.1,attrs==19.3.0,bowler==0.8.0,bsdiff4==1.1.5,Cerberus==1.3.2,certifi==2019.11.28,chardet==3.0.4,Click==7.0,coverage==4.5.4,dephell==0.7.9,dephell-archive==0.1.5,dephell-discover==0.2.10,dephell-licenses==0.1.6,dephell-links==0.1.4,dephell-markers==1.0.2,dephell-pythons==0.1.12,dephell-setuptools==0.2.1,dephell-shells==0.1.3,dephell-specifier==0.2.1,dephell-venvs==0.1.17,dephell-versioning==0.1.1,docker==4.1.0,dockerpty==0.4.1,docutils==0.15.2,dsdev-utils==1.0.4,ed25519==1.5,execnet==1.7.1,fissix==19.2b1,Flask==1.1.1,flatdict==3.4.0,gitdb2==2.0.6,GitPython==3.0.5,html5lib==1.0.1,idna==2.8,importlib-metadata==1.1.0,isort==4.3.21,itsdangerous==1.1.0,Jinja2==2.10.3,kubectl-val==0.1.4,lazy-object-proxy==1.4.3,logzero==1.5.0,m2r==0.2.1,MarkupSafe==1.1.1,mccabe==0.6.1,mistune==0.8.4,more-itertools==8.0.0,multidict==4.6.1,packaging==19.2,pbr==5.4.4,pexpect==4.7.0,pluggy==0.13.1,poodle==0.1.10,ptyprocess==0.6.0,py==1.8.0,Pygments==2.5.2,PyInstaller==3.5,pylint==2.4.4,pyparsing==2.4.5,pytest==5.3.1,pytest-cache==1.0,pytest-cov==2.8.1,pytest-pylint==0.14.1,PyUpdater==3.1.1,PyYAML==5.2,requests==2.22.0,ruamel.yaml==0.16.5,ruamel.yaml.clib==0.2.0,sh==1.12.14,shellingham==1.3.1,six==1.13.0,smmap2==2.0.5,stevedore==1.31.0,tabulate==0.8.6,tomlkit==0.5.8,typed-ast==1.4.0,urllib3==1.25.7,wcwidth==0.1.7,webencodings==0.5.1,websocket-client==0.56.0,Werkzeug==0.16.0,wrapt==1.11.2,yarl==1.4.1,yaspin==0.15.0,zipp==0.6.0\n",
      "poodledev run-test-pre: PYTHONHASHSEED='4247731550'\n",
      "poodledev run-test: commands[0] | bash -c 'cd ../poodle && dephell project build --from pyproject.toml'\n",
      "WARNING cannot find tool.dephell section in the config (path=pyproject.toml)\n",
      "INFO dumping... (format=setuppy)\n",
      "INFO dumping... (format=egginfo)\n",
      "INFO dumping... (format=sdist)\n",
      "INFO dumping... (format=wheel)\n",
      "INFO builded\n",
      "poodledev run-test: commands[1] | pip uninstall -y poodle\n",
      "Uninstalling poodle-0.1.10:\n",
      "  Successfully uninstalled poodle-0.1.10\n",
      "poodledev run-test: commands[2] | bash -c 'cd ../poodle && python ./setup.py install'\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing poodle.egg-info/PKG-INFO\n",
      "writing dependency_links to poodle.egg-info/dependency_links.txt\n",
      "writing entry points to poodle.egg-info/entry_points.txt\n",
      "writing requirements to poodle.egg-info/requires.txt\n",
      "writing top-level names to poodle.egg-info/top_level.txt\n",
      "reading manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "writing manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/poodle_main.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/schedule.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/arithmetic.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/pddlSplitter.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/web_solver.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/__init__.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/problem.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/poodle_main.py to poodle_main.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/schedule.py to schedule.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/arithmetic.py to arithmetic.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/pddlSplitter.py to pddlSplitter.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/web_solver.py to web_solver.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/problem.py to problem.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "poodle.__pycache__.poodle_main.cpython-37: module references __file__\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getsourcefile\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.findsource\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getframeinfo\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getouterframes\n",
      "creating 'dist/poodle-0.1.10-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing poodle-0.1.10-py3.7.egg\n",
      "creating /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Extracting poodle-0.1.10-py3.7.egg to /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Adding poodle 0.1.10 to easy-install.pth file\n",
      "Installing poodleserver script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Installed /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Processing dependencies for poodle==0.1.10\n",
      "Searching for requests==2.22.0\n",
      "Best match: requests 2.22.0\n",
      "Adding requests 2.22.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Flask==1.1.1\n",
      "Best match: Flask 1.1.1\n",
      "Adding Flask 1.1.1 to easy-install.pth file\n",
      "Installing flask script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for urllib3==1.25.7\n",
      "Best match: urllib3 1.25.7\n",
      "Adding urllib3 1.25.7 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for chardet==3.0.4\n",
      "Best match: chardet 3.0.4\n",
      "Adding chardet 3.0.4 to easy-install.pth file\n",
      "Installing chardetect script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for idna==2.8\n",
      "Best match: idna 2.8\n",
      "Adding idna 2.8 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for certifi==2019.11.28\n",
      "Best match: certifi 2019.11.28\n",
      "Adding certifi 2019.11.28 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Werkzeug==0.16.0\n",
      "Best match: Werkzeug 0.16.0\n",
      "Adding Werkzeug 0.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Click==7.0\n",
      "Best match: Click 7.0\n",
      "Adding Click 7.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Jinja2==2.10.3\n",
      "Best match: Jinja2 2.10.3\n",
      "Adding Jinja2 2.10.3 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for itsdangerous==1.1.0\n",
      "Best match: itsdangerous 1.1.0\n",
      "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for MarkupSafe==1.1.1\n",
      "Best match: MarkupSafe 1.1.1\n",
      "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Finished processing dependencies for poodle==0.1.10\n",
      "poodledev run-test: commands[3] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "CLEAN\n",
      "poodledev run-test: commands[4] | bash -c 'cd ../downward && timeout 40000 poodleserver 2>&1 >/dev/null &'\n",
      "poodledev run-test: commands[5] | python -m pytest -vv --durations=100 --disable-pytest-warnings --pylint --pylint-jobs=4 --pylint-error-types=EF -s -x tests/test_synt_affinity.py::test_7_3nodes_and_needed3nodes_suggests_movement_of_pods\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.5, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /home/vasily/artem/kubectl-val/.tox/poodledev/bin/python\n",
      "cachedir: .tox/poodledev/.pytest_cache\n",
      "rootdir: /home/vasily/artem/kubectl-val\n",
      "plugins: pylint-0.14.1, cov-2.8.1\n",
      "collecting ... [D 191208 09:58:03 libs_for_tests:31] hello\n",
      "collected 1 item\n",
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_7_3nodes_and_needed3nodes_suggests_movement_of_pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
